import gym
import numpy as np
import tensorflow as tf


#################
# Configuration #
#################

environment_name = 'CartPole-v0'
num_epochs = 50
min_steps_per_epoch = 5000
learning_rate = 1e-2


#####################
# Setup Environemnt #
#####################

environment = gym.make(environment_name)
num_actions = environment.action_space.n
observation_dimensions = np.prod(environment.observation_space.shape)


################
# Setup Policy #
################

# We need to set up placeholders for input to the policy.
observation_placeholder = tf.placeholder(shape=(None, observation_dimensions), dtype=tf.float32)
action_placeholder = tf.placeholder(shape=(None,), dtype=tf.int32)
weights_placeholder = tf.placeholder(shape=(None,), dtype=tf.float32)

# Build a feedforward network (MLP) representing the policy, which produces an
# un-normalized probability distribution in the output layer (aka one-hot
# encoding), given observations in the input layer.
previous_layer = observation_placeholder
hidden_layers = [32]
for hidden_layer in hidden_layers:
    previous_layer = tf.layers.dense(previous_layer, units=hidden_layer, activation=tf.tanh)
output_layer = tf.layers.dense(previous_layer, units=num_actions, activation=None)
logits = output_layer


# Build an operation for sampling an action from the stochastic probability
# distrubtion produced by the output layer of the MLP. This is an example of
# the discrete choice problem.
#
# Here, squeeze, just drops axies from the output shape that onely have one
# dimension.
action_operation = tf.squeeze(tf.multinomial(logits=logits, num_samples=1), axis=1)


# Build the psuedo_loss function.

# Returns a one hot encoded vector for each of the indices in action_placeholder. This means
# if we have 3 indices/actions, then this will return a ndarray with shape (3,3).
action_masks = tf.one_hot(indices=action_placeholder, depth=num_actions)

# Compute the log_probability of taking action a_t given observation o_t. This
# is equivalent to the log of the stochastic policy, which is the probability distribution ...
# Use log_softmax to transform/normalize the output of the MLP into the log probability distribution over actions.
#
# We want to sum across axis 1 only,
log_probabilities = tf.reduce_sum(action_masks * tf.nn.log_softmax(logits), axis=1)

# Build the psuedo loss function as an approximation of the expected value of taking action a weighted by some factor.
# An example factor and the one we'll use in this basic policy gradient optimization example is the return for the
# trajectory that the action was used in. Reduce mean approximates this expected value given that we only have a finite
# number of samples. Since we want to turn this into gradient descent and the expected value will be more positive for
# a better performing policy, we need to negate that it.
psuedo_loss = -tf.reduce_mean(weights_placeholder * log_probabilities)


# Build the training operation, which uses the gradient of the pseudo loss to update the policy.
training_operation = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(psuedo_loss)


# Initialize tensorflow.
session = tf.InteractiveSession()
session.run(tf.global_variables_initializer())


################
# Train Policy #
################

for epoch in range(num_epochs):

    ########################
    # Gather Training Data #
    ########################

    # Initialize lists to hold training data.
    batch_observations = []
    batch_actions = []
    batch_weights = []

    # Initialize lists for diagnostic information.
    batch_episode_returns = []
    batch_episode_lengths = []

    # Initialize a list to track rewards for the current episode.
    episode_rewards = []

    # Initialize the environment.
    observation = environment.reset()

    while True:

        # Sample an action from our policy given the last observation.
        action = session.run(
            action_operation,
            feed_dict={
                observation_placeholder: np.array(observation.reshape(1,-1))
            })[0]

        # Record the observation and action as training data.
        batch_observations.append(observation)
        batch_actions.append(action)

        # Step the environment, taking the action generated by the policy.
        observation, reward, done, info = environment.step(action)

        # Record the reward recieved for taking the action given the observation.
        episode_rewards.append(reward)

        if done:

            # Compute the weights that will be used to update the policy for the
            # newly finished episode. We need a weight for each (observation,
            # action) pair recorded so far.
            # Use the reward to go as the weights.
            episode_return = sum(episode_rewards)
            episode_length = len(episode_rewards)
            batch_weights += np.cumsum(episode_rewards[::-1])[::-1].tolist()

            # Record diagnostic information.
            batch_episode_returns.append(episode_return)
            batch_episode_lengths.append(episode_length)

            # Done gather training data for this epoch if we've aquired the
            # minimum number of training examples.
            if len(batch_observations) > min_steps_per_epoch:
                break;

            # Re-initialize the list to track rewards for the current episode.
            episode_rewards = []

            # Re-initialize the environment.
            observation = environment.reset()

    #################
    # Update Policy #
    #################

    batch_loss, _ = session.run(
        [
            psuedo_loss,
            training_operation
        ],
        feed_dict={
            observation_placeholder: np.array(batch_observations),
            action_placeholder: np.array(batch_actions),
            weights_placeholder: np.array(batch_weights)
        })


    ####################
    # Display Progress #
    ####################

    print("epoch: %3d\tloss: %.3f\treturn: %.3f\tepisode length: %.3f" %
        (epoch, batch_loss, np.mean(batch_episode_returns), np.mean(batch_episode_lengths)))
